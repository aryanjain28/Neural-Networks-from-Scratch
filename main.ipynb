{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 2\n",
    "# Coding a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a array/vector that will either be\n",
    "# an actual training data or output of neurons from prev layer.\n",
    "inputs = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input will also have a weight associated with it,\n",
    "# this weight will be tuned later while training.\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "\n",
    "# Next is bias, each neuron has just one bias value.\n",
    "# Therefore, for now, our single neuron will have just one biasValue\n",
    "bias = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  2.3\n"
     ]
    }
   ],
   "source": [
    "output = (inputs[0]*weights[0] +\n",
    "          inputs[1]*weights[1] +\n",
    "          inputs[2]*weights[2] + bias)\n",
    "\n",
    "print(\"Output: \", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer of Neurons\n",
    "\n",
    "Each neuron in a layer takes exactly the same input — the input given to the layer (which can be either the training data or the output from the previous layer), but contains its own set of weights and its own bias, producing its own unique output. The layer’s output is a set of each of these outputs — one per each neuron. \n",
    "\n",
    "![alt text](imagename.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  [4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# A neural layer with 3 neurons\n",
    "\n",
    "inputs = [1,2,3,2.5]\n",
    "\n",
    "# for each neuron we will have a set of weights\n",
    "# in our case, 3 arrays of weights.\n",
    "# remember that each weight value is associated with each input value\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "# for each neuron we will have a single bias value\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "outputs = [\n",
    "    # output for neuron 1\n",
    "    inputs[0]*weights1[0] + \n",
    "    inputs[1]*weights1[1] + \n",
    "    inputs[2]*weights1[2] +\n",
    "    inputs[3]*weights1[3] +\n",
    "    bias1,\n",
    "\n",
    "    # output for neuron 2\n",
    "    inputs[0]*weights2[0] +\n",
    "    inputs[1]*weights2[1] +\n",
    "    inputs[2]*weights2[2] +\n",
    "    inputs[3]*weights2[3] +\n",
    "    bias2,\n",
    "\n",
    "    # output for neuron 3\n",
    "    inputs[0]*weights3[0] +\n",
    "    inputs[1]*weights3[1] +\n",
    "    inputs[2]*weights3[2] +\n",
    "    inputs[3]*weights3[3] +\n",
    "    bias3\n",
    "]\n",
    "\n",
    "print(\"Output: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we have three sets of weights and three biases, which define three neurons. Each neuron is “connected” to the same inputs. The difference is in the separate weights and bias that each neuron applies to the input. This is called a fully connected neural network — every neuron in the current layer has connections to every neuron from the previous layer. This is a very common type of neural network, but it should be noted that there is no requirement to fully connect everything like this. At this point, we have only shown code for a single layer with very few neurons. Imagine coding many more layers and more neurons. This would get very challenging to code using our current methods. Instead, we could use a loop to scale and handle dynamically-sized inputs and layers. We’ve turned the separate weight variables into a list of weights so we can iterate over them, and we changed the code to use loops instead of the hardcoded operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1],            #weights for #1 neuron\n",
    "    [0.5, -0.91, 0.26, -0.5],       #weights for #2 neuron\n",
    "    [-0.26, -0.27, 0.17, 0.87]      #weights for #3 neuron\n",
    "]\n",
    "bias = [2, 3, 0.5]\n",
    "\n",
    "# output for current layer\n",
    "outputs = []\n",
    "for weightsOfNeuron, biasOfNeuron in zip(weights, bias):\n",
    "    output = 0\n",
    "    for weightValue, inputValue in zip(weightsOfNeuron, inputs):\n",
    "        output += inputValue*weightValue\n",
    "    outputs.append(output + biasOfNeuron)\n",
    "    \n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "# A single Neuron with NumPy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights= [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "outputs = np.dot(weights, inputs) + bias\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n",
      "[4.8, 1.2099999999999997, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# A layer of Neurons with NumPy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1],            #weights for #1 neuron\n",
    "    [0.5, -0.91, 0.26, -0.5],       #weights for #2 neuron\n",
    "    [-0.26, -0.27, 0.17, 0.87]      #weights for #3 neuron\n",
    "]\n",
    "bias = [2, 3, 0.5]\n",
    "\n",
    "# conventional\n",
    "outputsFirstWay = []\n",
    "for i in range(len(weights)): outputsFirstWay.append(np.dot(inputs, weights[i]) + bias[i])\n",
    "print(outputsFirstWay)\n",
    "\n",
    "# using np\n",
    "print(list(np.dot(weights, inputs) + bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batches : Many samples at once\n",
    "\n",
    "But why?\n",
    "Because first, it’s faster to train in batches in parallel processing, and second, batches help with generalization during training.\n",
    "\n",
    "If you fit (perform a step of a training process) on one sample at a time, you’re highly likely to keep fitting to that individual sample, rather than slowly producing general tweaks to weights and biases that fit the entire dataset. Fitting or training in batches gives you a higher chance of making more meaningful changes to weights and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([[1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "print(np.expand_dims(np.array(a), axis=0))\n",
    "print(np.expand_dims(np.array(a), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20]]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [2,3,4]\n",
    "\n",
    "a = np.array([a])\n",
    "b = np.array([b]).T\n",
    "\n",
    "print(np.dot(a, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "# A Layer of Neurons & Batch of Data w/ NumPy\n",
    "\n",
    "inputs = [\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "]\n",
    "\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "inputsArr = np.array(inputs)\n",
    "weightsArr = np.array(weights).T\n",
    "\n",
    "print(np.dot(inputsArr, weightsArr) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41e1e3c7e81f3e7867854638197c762adbafb8b4b2cb2c857e1d6479227fdb41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf-gpu-v1.14': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
