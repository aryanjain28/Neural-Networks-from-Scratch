{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter - 8 : Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "<img src=\"backPropFormula.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1 1 1 1.0\n",
      "-3.0 -1.0 2.0 1.0 -2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# We will write code for backpropagation method\n",
    "# Recall using CHAIN RULE, we find derivative of a function w.r.t the function inside it.\n",
    "# We store this result, called gradient, and use it to multiply to the derivative to previous\n",
    "# layer's output.\n",
    "\n",
    "# For simplicity we are going to consider that the gradient we recieved from the\n",
    "# next layer is 1, since multiplying with one wont change anything\n",
    "\n",
    "# Let's code now\n",
    "\n",
    "# FORWARD PASS\n",
    "x = [1.0, -2.0, 3.0]    # inputs\n",
    "w = [-3.0, -1.0, 2.0]   # weights\n",
    "b = 1.0                 # bias\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# output of dense layer\n",
    "sum = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU function\n",
    "y = max(sum, 0)\n",
    "\n",
    "# BACKWARD PASS\n",
    "\n",
    "# The derivative from previous layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# One important thing to note here is that the derivative of ReLU function is\n",
    "# 1 if the input is greater than 1 else 0\n",
    "drelu_dsum = dvalue * (1.0 if sum > 0 else 0.0)\n",
    "print(drelu_dsum)\n",
    "\n",
    "# Another important thing to note is that the partial derivative\n",
    "# of a sum is always 1 no matter the inputs\n",
    "\n",
    "# for x0w0 pair\n",
    "dsum_dmulxw0 = 1\n",
    "drelu_dmulxw0 = drelu_dsum * dsum_dmulxw0\n",
    "\n",
    "# for x1w1 pair\n",
    "dsum_dmulxw1 = 1\n",
    "drelu_dmulxw1 = drelu_dsum * dsum_dmulxw1\n",
    "\n",
    "# for x2w2 pair\n",
    "dsum_dmulxw2 = 1\n",
    "drelu_dmulxw2 = drelu_dsum * dsum_dmulxw2\n",
    "\n",
    "# for b\n",
    "dsum_db = 1\n",
    "drelu_db = drelu_dsum * dsum_db\n",
    "\n",
    "print(dsum_dmulxw0, dsum_dmulxw1, dsum_dmulxw2, drelu_db)\n",
    "\n",
    "# Continuing with the backpropagation\n",
    "# One more important thing to note here is that the partial derivative of\n",
    "# a product is the value with which it is being multiplied.\n",
    "# For eg.\n",
    "# d(x*y)/d(x) = y\n",
    "# d(x*y)/d(y) = x\n",
    "# Therefore, partial derivative of the first weighted-input w.r.t the input equals the weight\n",
    "\n",
    "# with respect to x values\n",
    "dmulxw0_dx0 = w[0]\n",
    "drelu_dx0 = drelu_dmulxw0 * dmulxw0_dx0\n",
    "\n",
    "dmulxw1_dx1 = w[1]\n",
    "drelu_dx1 = drelu_dmulxw1 * dmulxw1_dx1\n",
    "\n",
    "dmulxw2_dx2 = w[2]\n",
    "drelu_dx2 = drelu_dmulxw2 * dmulxw2_dx2\n",
    "\n",
    "# with respect to w values (weights)\n",
    "dmulxw0_dw0 = x[0]\n",
    "drelu_dw0 = drelu_dmulxw0 * dmulxw0_dw0\n",
    "\n",
    "dmulxw1_dw1 = x[1]\n",
    "drelu_dw1 = drelu_dmulxw1 * dmulxw1_dw1\n",
    "\n",
    "dmulxw2_dw2 = x[2]\n",
    "drelu_dw2 = drelu_dmulxw2 * dmulxw2_dw2\n",
    "print(drelu_dx0, drelu_dx1, drelu_dx2, drelu_dw0, drelu_dw1, drelu_dw2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./drelu_dz_final1.png\" height=\"300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above completes out code for backpropagation for a single neuron.\n",
    "# We can optimize the code a bit\n",
    "\n",
    "drelu_dx0 = drelu_dmulxw0 * dmulxw0_dx0\n",
    "# here\n",
    "dmulxw0_dx0 = w[0]\n",
    "# Therefore,\n",
    "drelu_dx0 = drelu_dmulxw0 * w[0]\n",
    "\n",
    "# Now\n",
    "drelu_dmulxw0 = drelu_dsum * dsum_dmulxw0\n",
    "\n",
    "# Then\n",
    "drelu_dx0 = drelu_dsum * dsum_dmulxw0 * w[0]\n",
    "\n",
    "# Here\n",
    "dsum_dmulxw0 = 1\n",
    "\n",
    "# Therefore\n",
    "drelu_dx0 = drelu_dsum * 1 * w[0]\n",
    "drelu_dx0 = drelu_dsum * w[0]\n",
    "\n",
    "# where\n",
    "drelu_dsum = dvalue * (1.0 if sum > 0 else 0.0)\n",
    "\n",
    "# Finally\n",
    "drelu_dx0 = dvalue * (1.0 if sum > 0 else 0.0) * w[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./optimized_form.png\" height=\"300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All together, the partial derivatives above, combined into a vector, make up our gradients. Our\n",
    "# gradients could be represented as:\n",
    "\n",
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]  # gradient vector for inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]  # gradient vector for weights\n",
    "db = drelu_db \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights and biases:  [-3.0, -1.0, 2.0] 1.0\n",
      "Weights after optimization:  [-3.001, -0.998, 1.997] 0.999\n",
      "New result:  5.985\n"
     ]
    }
   ],
   "source": [
    "# Moving forward, we will discuss briefly about optimizers.\n",
    "\n",
    "print(\"Original weights and biases: \", w, b)\n",
    "\n",
    "# We then add a negative fraction of our gradients to these weights and biases\n",
    "# and try to optimize it.\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "# We are applying -ve fraction because we want to decrease the overall value\n",
    "print(\"Weights after optimization: \", w, b)\n",
    "\n",
    "# Now we'll observe the change that occurend due to out calculations\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "sum = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU Activation Function\n",
    "y = max(sum, 0)\n",
    "print(\"New result: \", y)\n",
    "\n",
    "# We’ve successfully decreased this neuron’s output from 6.000 to 5.985.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since now we've considered the case of a single neuron for the sake \n",
    "# of understanding.\n",
    "# Now we'll move ahead and write code for a proper network with \n",
    "# array of inputs and weights.\n",
    "\n",
    "# Now, let’s replace the current singular neuron with a layer of neurons. As opposed to a single\n",
    "# neuron, a layer outputs a vector of values instead of a singular value. Each neuron in a layer\n",
    "# connects to all of the neurons in the next layer. During backpropagation, each neuron from the\n",
    "# current layer will receive a vector of partial derivatives the same way that we described for a\n",
    "# single neuron. With a layer of neurons, it’ll take the form of a list of these vectors, or a 2D array.\n",
    "# We know that we need to perform a sum, but what should we sum and what is the result supposed\n",
    "# to be? Each neuron is going to output a gradient of the partial derivatives with respect to all of its\n",
    "# inputs, and all neurons will form a list of these vectors. We need to sum along the inputs — the\n",
    "# first input to all of the neurons, the second input, and so on. We’ll have to sum columns.\n",
    "\n",
    "# To calculate the partial derivatives with respect to inputs, we need the weights — the partial\n",
    "# derivative with respect to the input equals the related weight. This means that the array of\n",
    "# partial derivatives with respect to all of the inputs equals the array of weights. Since this array is\n",
    "# transposed, we’ll need to sum its rows instead of columns. To apply the chain rule, we need to\n",
    "# multiply them by the gradient from the subsequent function.\n",
    "\n",
    "# In the code to show this, we take the transposed weights, which are the transposed array of the\n",
    "# derivatives with respect to inputs, and multiply them by their respective gradients (related to\n",
    "# given neurons) to apply the chain rule. Then we sum along with the inputs. Then we calculate\n",
    "# the gradient for the next layer in backpropagation. The “next” layer in backpropagation is the\n",
    "# previous layer in the order of creation of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:  [[ 0.44 -0.38 -0.07  1.37]]\n"
     ]
    }
   ],
   "source": [
    "# Backward Propagation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# These are the gradients passed from the next layer.\n",
    "# Note: gradient is a vector which has values equal to the\n",
    "# neurons in the previous layer\n",
    "# Eg. We have 3 neurons in our current layer, therefore, the gradients will\n",
    "# contain 3 values.\n",
    "dvalues = np.array([[1.0, 1.0, 1.0]])\n",
    "\n",
    "# 3 neurons with 4 weights\n",
    "weights = np.array([\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "])\n",
    "\n",
    "# We know that the derivative of inputs is the related weight.\n",
    "# Therefore we transpose weights\n",
    "# gradients for the current layer\n",
    "inputs = weights.T\n",
    "dinputs = np.dot(dvalues, inputs.T)\n",
    "\n",
    "print(\"Gradients: \", dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t inputs:\n",
      "  [[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "# in case of batch-of-data\n",
    "import numpy as np\n",
    "\n",
    "# 3 batches of inputs\n",
    "dvalues = np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0],\n",
    "])\n",
    "\n",
    "# 3 neurons with 4 weights\n",
    "weights = np.array([\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "])\n",
    "\n",
    "inputs = weights\n",
    "dinputs = np.dot(dvalues, inputs)\n",
    "\n",
    "print(\"Gradients w.r.t inputs:\\n \", dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t inputs:\n",
      "  [[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "# similary for dweights\n",
    "import numpy as np\n",
    "\n",
    "# 3 batches of inputs\n",
    "dvalues = np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0],\n",
    "])\n",
    "\n",
    "inputs = np.array([\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2, 5, -1, 2],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "weights = inputs\n",
    "dweights = np.dot(weights.T, dvalues)\n",
    "\n",
    "print(\"Gradients w.r.t inputs:\\n \", dweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t biases:\n",
      "  [[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# similarly for biases\n",
    "\n",
    "# For the biases and derivatives with respect to them, the derivatives come from the sum operation\n",
    "# and always equal 1, multiplied by the incoming gradients to apply the chain rule. Since gradients\n",
    "# are a list of gradients (a vector of gradients for each neuron for all samples), we just have to sum\n",
    "# them with the neurons, column-wise, along axis 0.\n",
    "\n",
    "# in case of batch-of-data\n",
    "import numpy as np\n",
    "\n",
    "# 3 batches of inputs\n",
    "dvalues = np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0],\n",
    "])\n",
    "\n",
    "# 3 neurons with 4 weights\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "dinputs = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "print(\"Gradients w.r.t biases:\\n \", dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "314aa30014bb153b034f19e69298040a25a5fd6da26c1ba4e72688d64b409217"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf_gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
