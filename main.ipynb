{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter-5 : Calculating Network Error with Loss </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Categorical Cross-Entropy Loss</h3>\n",
    "\n",
    "\n",
    "<img src=\"./lossFunc.png\"/>\n",
    "\n",
    "Where Li denotes sample loss value, i is the i-th sample in the set, \n",
    "j is the label/output index, y denotes the target values, and y-hat denotes the predicted values.\n",
    "\n",
    "A classification model (like that of ours), returns a probability distributions over all the outputs.<br/>\n",
    "Cross-Entropy compares two probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the below arr as the output of out model\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# consider that the desired prediction is the first index, then probablity distribution will be like-\n",
    "desired_ouput = [1, 0, 0]\n",
    "#Thea above is also called as the one-hot vector, in which the index of desired-output class is 1 and others are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./lossFunc2.png\"/>\n",
    "\n",
    "This is how the categorical-cross-entropy loss function works.\n",
    "<br/>\n",
    "Let's implement this in a Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.35667494393873245\n",
      "New Loss:  0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(\n",
    "    math.log(softmax_output[0])*target_output[0] +\n",
    "    math.log(softmax_output[1])*target_output[1] +\n",
    "    math.log(softmax_output[2])*target_output[2] \n",
    ")\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "\n",
    "# This is the full categorical cross entropy calculation.\n",
    "# Furthermore, value of target_output[1] and  target_output[2] is 0, so we need not to calculate that.\n",
    "# Also, value of target_output[0] is 1.\n",
    "# Therefore, the below gives us the same output\n",
    "\n",
    "new_loss = -(math.log(softmax_output[0]))\n",
    "print(\"New Loss: \", new_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-0.10536051565782628\n",
      "-0.2231435513142097\n",
      "-0.35667494393873245\n",
      "-0.5108256237659907\n",
      "-0.6931471805599453\n",
      "...\n",
      "-2.3025850929940455\n",
      "-2.995732273553991\n",
      "-4.605170185988091\n"
     ]
    }
   ],
   "source": [
    "# One important feature regarding Cross Entropy Function\n",
    "output_1 = [0.22, 0.6, 0.18]\n",
    "output_2 = [0.32, 0.36, 0.32]\n",
    "\n",
    "# In both the above cases, the argmax (returns the index of highest value in array),\n",
    "# will return us 1 (index of -> 0.6 and 0.36 {HIGHEST})\n",
    "# But the confidence of our model is far very less in the case of output_2.\n",
    "# Therefore, the Categorical Cross-Entropy Loss accounts for this and outputs a larger loss in the case of lower confidence\n",
    "# Let's see how\n",
    "\n",
    "print(math.log(1.))\n",
    "print(math.log(0.9))\n",
    "print(math.log(0.8))\n",
    "print(math.log(0.7))\n",
    "print(math.log(0.6))\n",
    "print(math.log(0.5))\n",
    "print(\"...\")\n",
    "print(math.log(0.1))\n",
    "print(math.log(0.05))\n",
    "print(math.log(0.01))\n",
    "\n",
    "# The different log values show the different loss values.\n",
    "# As noted, log(1) = 0, this means that the model is 100% sure about its prediction and that the loss function is 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Batches of input\n",
    "softmax_output = np.array([\n",
    "    [0.7, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.4],\n",
    "    [0.02, 0.9, 0.08]\n",
    "])\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "# np gives us independence to get values from a list of indexes\n",
    "correct_confidence = softmax_output[range(len(softmax_output)), class_targets]\n",
    "print(correct_confidence)\n",
    "\n",
    "# Applying -ve log to these values\n",
    "neg_loss = -np.log(correct_confidence)\n",
    "print(neg_loss)\n",
    "\n",
    "# Avg loss\n",
    "avg_loss = np.mean(neg_loss)\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# The above is the case when the target_values are sparse, \n",
    "# meaning, the values in array contain the correct class numbers\n",
    "# However the target_values can also be one-hot coded and we have to handle seperately.\n",
    "\n",
    "import math\n",
    "\n",
    "softmax_output = np.array([\n",
    "    [0.7, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.4],\n",
    "    [0.02, 0.9, 0.08]\n",
    "])\n",
    "\n",
    "class_targets = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "correct_confidence = np.sum(softmax_output*class_targets, axis=1)\n",
    "print(correct_confidence)\n",
    "\n",
    "# Rest is same as above\n",
    "# Applying -ve log to these values\n",
    "neg_loss = -np.log(correct_confidence)\n",
    "print(neg_loss)\n",
    "\n",
    "# Avg loss\n",
    "avg_loss = np.mean(neg_loss)\n",
    "print(avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a problem with log(0) = INF | Undefined.\n",
    "# Therefore we clip values from both end in a log function\n",
    "\n",
    "# y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Loss Function -> Will be used by all the different types of loss functions\n",
    "# Returns an avg of losses\n",
    "class Loss:\n",
    "\n",
    "    # output => model's prediction\n",
    "    # y => ground truth\n",
    "    def calculate(self, output, y):\n",
    "        # forward method is of specific loss function eg. Cross Entropy\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss:\n",
    "class Loss_Categorical_Cross_Entropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # check if y_true is sparse or one-hot-coded\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(len(y_pred_clipped)), y_true]\n",
    "        else:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        neg_log = -np.log(correct_confidence)\n",
    "        return neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_output = np.array([\n",
    "    [0.7, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.4],\n",
    "    [0.02, 0.9, 0.08]\n",
    "])\n",
    "\n",
    "class_targets = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "loss_function = Loss_Categorical_Cross_Entropy()\n",
    "loss = loss_function.calculate(softmax_output, class_targets)\n",
    "\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Code upto this point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
