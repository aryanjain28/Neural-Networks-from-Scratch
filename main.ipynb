{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 4: Activation Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will tackle a few of the activation functions and discuss their roles. We use\n",
    "different activation functions for different cases, and understanding how they work can help you\n",
    "properly pick which of them is best for your task. The activation function is applied to the output\n",
    "of a neuron (or layer of neurons), which modifies outputs. We use activation functions because if\n",
    "the activation function itself is nonlinear, it allows for neural networks with usually two or more\n",
    "hidden layers to map nonlinear functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Types of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\"> Step Activation Function </font>\n",
    "<br />\n",
    "<br />\n",
    "if the weights · inputs + bias results in a value greater than 0, the neuron will fire and output a 1;\n",
    "otherwise, it will output a 0.\n",
    "<br />\n",
    "<br />\n",
    "<img src=\"./stepActivation.png\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\"> Linear Activation Function</font>\n",
    "<br />\n",
    "(basically a straight line)\n",
    "<br />\n",
    "<br />\n",
    "This activation function is usually applied to the last layer’s output in the case of a regression\n",
    "model — a model that outputs a scalar value instead of a classification.\n",
    "<br />\n",
    "<br />\n",
    "<img src=\"linearActivation.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\"> Sigmoid Activation Function</font>\n",
    "<br />\n",
    "<br />\n",
    "This function returns a value in the range of 0 for negative infinity, through 0.5 for the input of 0,\n",
    "and to 1 for positive infinity. \n",
    "<br />\n",
    "<br />\n",
    "<img src=\"sigmoidActivation.png\" />\n",
    "\n",
    "\n",
    "The output from the Sigmoid function, being in the range of 0 to 1, also works better\n",
    "with neural networks — especially compared to the range of the negative to the positive infinity\n",
    "— and adds nonlinearity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\"> Rectified Linear Activation Function</font>\n",
    "<br />\n",
    "<br />\n",
    "The rectified linear activation function is simpler than the sigmoid. It’s quite literally y=x, clipped at 0 from the negative side. If x is less than or equal to 0, then y is 0 — otherwise, y is equal to x\n",
    "<br />\n",
    "<br />\n",
    "<img src=\"reluActivation.png\" />\n",
    "\n",
    "\n",
    "This simple yet powerful activation function is the most widely used activation function at the time of writing for various reasons — mainly speed and efficiency. While the sigmoid activation function isn’t the most complicated, it’s still much more challenging to compute than the ReLU\n",
    "activation function. The ReLU activation function is extremely close to being a linear activation function while remaining nonlinear, due to that bend after 0. This simple property is, however, very effective. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation function\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "\n",
    "# Convention\n",
    "for i in inputs:\n",
    "    output.append(max(i, 0))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.maximum(0, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.1\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "print(nnfs.__version__)\n",
    "nnfs.init()\n",
    "\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41e1e3c7e81f3e7867854638197c762adbafb8b4b2cb2c857e1d6479227fdb41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf-gpu-v1.14': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
