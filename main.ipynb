{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter - 8 : Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "<img src=\"backPropFormula.png\"/>\n",
    "<h6>If you are reading this, I don't want to scare you but this topic can be a bit hard to understand. Read it again and again, if you still find it difficult, mail me, I can help you.</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1 1 1 1.0\n",
      "-3.0 -1.0 2.0 1.0 -2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# We will write code for backpropagation method\n",
    "# Recall using CHAIN RULE, we find derivative of a function w.r.t the function inside it.\n",
    "# We store this result, called gradient, and use it to multiply to the derivative to previous\n",
    "# layer's output.\n",
    "\n",
    "# For simplicity we are going to consider that the gradient we recieved from the\n",
    "# next layer is 1, since multiplying with one wont change anything\n",
    "\n",
    "# Let's code now\n",
    "\n",
    "# FORWARD PASS\n",
    "x = [1.0, -2.0, 3.0]    # inputs\n",
    "w = [-3.0, -1.0, 2.0]   # weights\n",
    "b = 1.0                 # bias\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# output of dense layer\n",
    "sum = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU function\n",
    "y = max(sum, 0)\n",
    "\n",
    "# BACKWARD PASS\n",
    "\n",
    "# The derivative from previous layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# One important thing to note here is that the derivative of ReLU function is\n",
    "# 1 if the input is greater than 1 else 0\n",
    "drelu_dsum = dvalue * (1.0 if sum > 0 else 0.0)\n",
    "print(drelu_dsum)\n",
    "\n",
    "# Another important thing to note is that the partial derivative\n",
    "# of a sum is always 1 no matter the inputs\n",
    "\n",
    "# for x0w0 pair\n",
    "dsum_dmulxw0 = 1\n",
    "drelu_dmulxw0 = drelu_dsum * dsum_dmulxw0\n",
    "\n",
    "# for x1w1 pair\n",
    "dsum_dmulxw1 = 1\n",
    "drelu_dmulxw1 = drelu_dsum * dsum_dmulxw1\n",
    "\n",
    "# for x2w2 pair\n",
    "dsum_dmulxw2 = 1\n",
    "drelu_dmulxw2 = drelu_dsum * dsum_dmulxw2\n",
    "\n",
    "# for b\n",
    "dsum_db = 1\n",
    "drelu_db = drelu_dsum * dsum_db\n",
    "\n",
    "print(dsum_dmulxw0, dsum_dmulxw1, dsum_dmulxw2, drelu_db)\n",
    "\n",
    "# Continuing with the backpropagation\n",
    "# One more important thing to note here is that the partial derivative of\n",
    "# a product is the value with which it is being multiplied.\n",
    "# For eg.\n",
    "# d(x*y)/d(x) = y\n",
    "# d(x*y)/d(y) = x\n",
    "# Therefore, partial derivative of the first weighted-input w.r.t the input equals the weight\n",
    "\n",
    "# with respect to x values\n",
    "dmulxw0_dx0 = w[0]\n",
    "drelu_dx0 = drelu_dmulxw0 * dmulxw0_dx0\n",
    "\n",
    "dmulxw1_dx1 = w[1]\n",
    "drelu_dx1 = drelu_dmulxw1 * dmulxw1_dx1\n",
    "\n",
    "dmulxw2_dx2 = w[2]\n",
    "drelu_dx2 = drelu_dmulxw2 * dmulxw2_dx2\n",
    "\n",
    "# with respect to w values (weights)\n",
    "dmulxw0_dw0 = x[0]\n",
    "drelu_dw0 = drelu_dmulxw0 * dmulxw0_dw0\n",
    "\n",
    "dmulxw1_dw1 = x[1]\n",
    "drelu_dw1 = drelu_dmulxw1 * dmulxw1_dw1\n",
    "\n",
    "dmulxw2_dw2 = x[2]\n",
    "drelu_dw2 = drelu_dmulxw2 * dmulxw2_dw2\n",
    "print(drelu_dx0, drelu_dx1, drelu_dx2, drelu_dw0, drelu_dw1, drelu_dw2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./drelu_dz_final1.png\" height=\"300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above completes out code for backpropagation for a single neuron.\n",
    "# We can optimize the code a bit\n",
    "\n",
    "drelu_dx0 = drelu_dmulxw0 * dmulxw0_dx0\n",
    "# here\n",
    "dmulxw0_dx0 = w[0]\n",
    "# Therefore,\n",
    "drelu_dx0 = drelu_dmulxw0 * w[0]\n",
    "\n",
    "# Now\n",
    "drelu_dmulxw0 = drelu_dsum * dsum_dmulxw0\n",
    "\n",
    "# Then\n",
    "drelu_dx0 = drelu_dsum * dsum_dmulxw0 * w[0]\n",
    "\n",
    "# Here\n",
    "dsum_dmulxw0 = 1\n",
    "\n",
    "# Therefore\n",
    "drelu_dx0 = drelu_dsum * 1 * w[0]\n",
    "drelu_dx0 = drelu_dsum * w[0]\n",
    "\n",
    "# where\n",
    "drelu_dsum = dvalue * (1.0 if sum > 0 else 0.0)\n",
    "\n",
    "# Finally\n",
    "drelu_dx0 = dvalue * (1.0 if sum > 0 else 0.0) * w[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./optimized_form.png\" height=\"300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All together, the partial derivatives above, combined into a vector, make up our gradients. Our\n",
    "# gradients could be represented as:\n",
    "\n",
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]  # gradient vector for inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]  # gradient vector for weights\n",
    "db = drelu_db \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights and biases:  [-3.0, -1.0, 2.0] 1.0\n",
      "Weights after optimization:  [-3.001, -0.998, 1.997] 0.999\n",
      "New result:  5.985\n"
     ]
    }
   ],
   "source": [
    "# Moving forward, we will discuss briefly about optimizers.\n",
    "\n",
    "print(\"Original weights and biases: \", w, b)\n",
    "\n",
    "# We then add a negative fraction of our gradients to these weights and biases\n",
    "# and try to optimize it.\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "# We are applying -ve fraction because we want to decrease the overall value\n",
    "print(\"Weights after optimization: \", w, b)\n",
    "\n",
    "# Now we'll observe the change that occurend due to out calculations\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "sum = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU Activation Function\n",
    "y = max(sum, 0)\n",
    "print(\"New result: \", y)\n",
    "\n",
    "# We’ve successfully decreased this neuron’s output from 6.000 to 5.985.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since now we've considered the case of a single neuron for the sake \n",
    "# of understanding.\n",
    "# Now we'll move ahead and write code for a proper network with \n",
    "# array of inputs and weights.\n",
    "\n",
    "# Now, let’s replace the current singular neuron with a layer of neurons. As opposed to a single\n",
    "# neuron, a layer outputs a vector of values instead of a singular value. Each neuron in a layer\n",
    "# connects to all of the neurons in the next layer. During backpropagation, each neuron from the\n",
    "# current layer will receive a vector of partial derivatives the same way that we described for a\n",
    "# single neuron. With a layer of neurons, it’ll take the form of a list of these vectors, or a 2D array.\n",
    "# We know that we need to perform a sum, but what should we sum and what is the result supposed\n",
    "# to be? Each neuron is going to output a gradient of the partial derivatives with respect to all of its\n",
    "# inputs, and all neurons will form a list of these vectors. We need to sum along the inputs — the\n",
    "# first input to all of the neurons, the second input, and so on. We’ll have to sum columns.\n",
    "\n",
    "# To calculate the partial derivatives with respect to inputs, we need the weights — the partial\n",
    "# derivative with respect to the input equals the related weight. This means that the array of\n",
    "# partial derivatives with respect to all of the inputs equals the array of weights. Since this array is\n",
    "# transposed, we’ll need to sum its rows instead of columns. To apply the chain rule, we need to\n",
    "# multiply them by the gradient from the subsequent function.\n",
    "\n",
    "# In the code to show this, we take the transposed weights, which are the transposed array of the\n",
    "# derivatives with respect to inputs, and multiply them by their respective gradients (related to\n",
    "# given neurons) to apply the chain rule. Then we sum along with the inputs. Then we calculate\n",
    "# the gradient for the next layer in backpropagation. The “next” layer in backpropagation is the\n",
    "# previous layer in the order of creation of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:  [[ 0.44 -0.38 -0.07  1.37]]\n"
     ]
    }
   ],
   "source": [
    "# Backward Propagation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# These are the gradients passed from the next layer.\n",
    "# Note: gradient is a vector which has values equal to the\n",
    "# neurons in the previous layer\n",
    "# Eg. We have 3 neurons in our current layer, therefore, the gradients will\n",
    "# contain 3 values.\n",
    "dvalues = np.array([[1.0, 1.0, 1.0]])\n",
    "\n",
    "# 3 neurons with 4 weights\n",
    "weights = np.array([\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "])\n",
    "\n",
    "# We know that the derivative of inputs is the related weight.\n",
    "# Therefore we transpose weights\n",
    "# gradients for the current layer\n",
    "inputs = weights.T\n",
    "dinputs = np.dot(dvalues, inputs.T)\n",
    "\n",
    "print(\"Gradients: \", dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t inputs:\n",
      "  [[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "# in case of batch-of-data\n",
    "import numpy as np\n",
    "\n",
    "# 3 batches of inputs\n",
    "dvalues = np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0],\n",
    "])\n",
    "\n",
    "# 3 neurons with 4 weights\n",
    "weights = np.array([\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "])\n",
    "\n",
    "inputs = weights\n",
    "dinputs = np.dot(dvalues, inputs)\n",
    "\n",
    "print(\"Gradients w.r.t inputs:\\n \", dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t inputs:\n",
      "  [[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "# similary for dweights\n",
    "import numpy as np\n",
    "\n",
    "# 3 batches of inputs\n",
    "dvalues = np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0],\n",
    "])\n",
    "\n",
    "inputs = np.array([\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2, 5, -1, 2],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "weights = inputs\n",
    "dweights = np.dot(weights.T, dvalues)\n",
    "\n",
    "print(\"Gradients w.r.t inputs:\\n \", dweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t biases:\n",
      "  [[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# similarly for biases\n",
    "\n",
    "# For the biases and derivatives with respect to them, the derivatives come from the sum operation\n",
    "# and always equal 1, multiplied by the incoming gradients to apply the chain rule. Since gradients\n",
    "# are a list of gradients (a vector of gradients for each neuron for all samples), we just have to sum\n",
    "# them with the neurons, column-wise, along axis 0.\n",
    "\n",
    "# in case of batch-of-data\n",
    "import numpy as np\n",
    "\n",
    "# 3 batches of inputs\n",
    "dvalues = np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0],\n",
    "])\n",
    "\n",
    "# 3 neurons with 4 weights\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "dinputs = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "print(\"Gradients w.r.t biases:\\n \", dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  0]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "# The last thing to cover here is the derivative of the ReLU function. It equals 1 if the input is\n",
    "# greater than 0 and 0 otherwise. The layer passes its outputs through the ReLU() activation during\n",
    "# the forward pass. For the backward pass, ReLU() receives a gradient of the same shape. The\n",
    "# derivative of the ReLU function will form an array of the same shape, filled with 1 when the\n",
    "# related input is greater than 0, and 0 otherwise\n",
    "\n",
    "z = np.array([\n",
    "    [1, 2, -3, -4],\n",
    "    [2, -7, -1, -3],\n",
    "    [-1, 2, 5, -1]\n",
    "])\n",
    "\n",
    "dvalues = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "\n",
    "drelu = np.zeros_like(z)\n",
    "drelu[z > 0] = 1\n",
    "\n",
    "print(drelu)\n",
    "\n",
    "drelu *= dvalues\n",
    "\n",
    "print(drelu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.179515   0.742093  -0.510153   0.971328 ]\n",
      " [ 0.5003665 -0.9152577  0.2529017 -0.5021842]\n",
      " [-0.262746  -0.2758402  0.1629592  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "# Full code with forward and backward propagation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2., 5., -1., 2],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs = np.dot(inputs, weights) + biases  # Dense layer\n",
    "relu = np.maximum(0, layer_outputs)  # ReLU activation\n",
    "\n",
    "# optimized...\n",
    "# drelu = relu_outputs.copy()\n",
    "# drelu[layer_outputs <= 0] = 0\n",
    "\n",
    "dinputs = np.dot(drelu, weights.T)      # shape => 3,4\n",
    "dweights = np.dot(inputs.T, drelu)      # shape => 3,4\n",
    "dbiases = np.sum(drelu, axis=0, keepdims=True)\n",
    "\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "\n",
    "print(weights.T)\n",
    "print(biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n",
      "[[ 11.   27.   -2.   12.5]\n",
      " [-13.   31.   39.   -3. ]\n",
      " [-16.5  29.7  36.3  -8.8]\n",
      " [  0.    0.    0.    0. ]]\n",
      "[[ 0.2   0.8  -0.5   1.  ]\n",
      " [ 0.5  -0.91  0.26 -0.5 ]\n",
      " [-0.26 -0.27  0.17  0.87]]\n"
     ]
    }
   ],
   "source": [
    "print(dinputs)\n",
    "print(dweights.T)\n",
    "print(weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Our full code upto this point</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "Avg Loss:  1.0986104\n",
      "Accuracy:  0.34\n"
     ]
    }
   ],
   "source": [
    "# Our Full Code\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer init\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation function\n",
    "class Activation_ReLU:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Softmax Activation Function\n",
    "class Activation_Softmax:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        expo_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        norm_values = expo_values / np.sum(expo_values, axis=1, keepdims=True)\n",
    "        self.output = norm_values\n",
    "\n",
    "# Common Loss\n",
    "class Loss:\n",
    "\n",
    "    # output => model's prediction\n",
    "    # y => ground truth\n",
    "    def calculate(self, output, y):\n",
    "        # forward method is of specific loss function eg. Cross Entropy\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross Entropy Loss:\n",
    "class Loss_Categorical_Cross_Entropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # check if y_true is sparse or one-hot-coded\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(len(y_pred_clipped)), y_true]\n",
    "        else:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        neg_log = -np.log(correct_confidence)\n",
    "        return neg_log\n",
    "\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Initialization\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_Categorical_Cross_Entropy()\n",
    "\n",
    "# Forward pass\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(\"Avg Loss: \", loss)\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "# outputs the index from softmax_output\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=2)\n",
    "\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# True evaluates to 1; False to 0\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making changes\n",
    "# During the forward method for our Layer_Dense class, we will want to remember what the\n",
    "# inputs were (we will need them during backprop)\n",
    "\n",
    "\n",
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer init\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        # saving inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradients\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU Activation Function\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, inputs):\n",
    "        # saving inputs\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "314aa30014bb153b034f19e69298040a25a5fd6da26c1ba4e72688d64b409217"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf_gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
